{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Twitter Scraping"
      ],
      "metadata": {
        "id": "THrq9oHFy9G7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJOxpmBgy686"
      },
      "outputs": [],
      "source": [
        "!pip3 install snscrape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import snscrape.modules.twitter as sntwitter"
      ],
      "metadata": {
        "id": "S8UNYj9WzOKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "topics = ['#OpenAI',\"#SARS-CoV-2\",\"#Chandrayaan3\",\"#INDvAUS\",\"#LPGPriceHike\",\"#BodhDiwas_Of_SantGaribdasJi\",\"#MPBudget2023\",\"#MadeForTrade\",\"#Messi\",\"AI\"]\n",
        "for i in topics:\n",
        "  scraper = sntwitter.TwitterSearchScraper(i)\n",
        "  c = 0\n",
        "  for tweet in scraper.get_items():\n",
        "    if c>=1000:\n",
        "      break;\n",
        "    data=[i,tweet.content]\n",
        "    tweets.append(data)\n",
        "    c= c+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60T5HCuLzZai",
        "outputId": "f9cb43d6-18d4-46fd-fd2c-0ae64259f01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b2854c020c48>:9: FutureWarning: content is deprecated, use rawContent instead\n",
            "  data=[i,tweet.content]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df = pd.DataFrame(tweets,columns=[\"topic\",\"content\"])"
      ],
      "metadata": {
        "id": "hvWIXB3B086j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping the tweets"
      ],
      "metadata": {
        "id": "LBW4q2nw1vVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modified_data=[]\n",
        "c = 0\n",
        "s = ''\n",
        "for i in range(len(tweets)):\n",
        "  s = s + tweet_df.loc[i]['content']\n",
        "  c= c+1\n",
        "  if c == 10:\n",
        "    data = [tweet_df.loc[i]['topic'],s]\n",
        "    modified_data.append(data)\n",
        "    c = 0\n",
        "    s = ''"
      ],
      "metadata": {
        "id": "1Pp_jM6W1fxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(modified_data,columns=[\"topic\",\"content\"])"
      ],
      "metadata": {
        "id": "YHGb7gcr18zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Processing"
      ],
      "metadata": {
        "id": "Zkk2molG2TPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "45U2CBxEtb04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/NLP.csv\")"
      ],
      "metadata": {
        "id": "eF-f1XA8tgjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower Casing"
      ],
      "metadata": {
        "id": "45SE3cMk2hq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: x.lower()) "
      ],
      "metadata": {
        "id": "s3MLA9Su2VYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating to English"
      ],
      "metadata": {
        "id": "c3Unn2Qn3cCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "Be7sDOdf3gmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator"
      ],
      "metadata": {
        "id": "kBId2HFg3sKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()\n",
        "def translate(x):\n",
        "  text_to_translate = translator.translate(x,dest= 'en')\n",
        "  return text_to_translate.text"
      ],
      "metadata": {
        "id": "IhWj4Z2b3yI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: translate(x))"
      ],
      "metadata": {
        "id": "RhbAkLwX4Zhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enpansion of Words"
      ],
      "metadata": {
        "id": "hPcR5Ixd46JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \" you \",\n",
        "\" ur \": \" your \",\n",
        "\" n \": \" and \"}\n",
        "def expand(x):\n",
        "        for key in contractions:\n",
        "            value = contractions[key]\n",
        "            x = x.replace(key, value)\n",
        "        return x;\n",
        "\n",
        "df['content'] = df['content'].apply(lambda x: expand(x))\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: expand(x))"
      ],
      "metadata": {
        "id": "uLH8W5QY3Jrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing URL"
      ],
      "metadata": {
        "id": "k0lcSKxl5bie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "eTrjpJYn5VmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x))\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x))"
      ],
      "metadata": {
        "id": "Cac6zAGE5jgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing RT"
      ],
      "metadata": {
        "id": "YRtr5jZc52o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: re.sub('RT', \"\", x))\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: re.sub('RT', \"\", x))\n"
      ],
      "metadata": {
        "id": "LnVaDQKK5zHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Emojis"
      ],
      "metadata": {
        "id": "8l-pwY1T6DkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)"
      ],
      "metadata": {
        "id": "wm9axY3T57Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: deEmojify(x))\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: deEmojify(x))\n"
      ],
      "metadata": {
        "id": "_N3HVJOg6K4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing HTML tags"
      ],
      "metadata": {
        "id": "qH_su1E06lJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "df['content'] = df['content'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())"
      ],
      "metadata": {
        "id": "qneYDX886a3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Special Characters and Punctuations"
      ],
      "metadata": {
        "id": "qxTFXITC7b4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = \"!”$%&’()*+-/:;<=>[]^_`{|}~•@#\"\n",
        "def removepunc(x):\n",
        "    return x.translate(str.maketrans('','',punctuation))"
      ],
      "metadata": {
        "id": "cP7N9W5C6rub"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: removepunc(x) )\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: removepunc(x) )"
      ],
      "metadata": {
        "id": "EdJ3P5Rt8DVD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removen(x):\n",
        "    return x.replace(\"\\n\",\"\")\n",
        "def removes(x):\n",
        "    return x.replace(\"&\",\"and\")"
      ],
      "metadata": {
        "id": "-tOJSqviB_We"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: removen(x) )\n",
        "df['content'] = df['content'].apply(lambda x: removes(x) )\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: removen(x) )\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: removes(x) )"
      ],
      "metadata": {
        "id": "w1MoKPcgCaQj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removal of Accented Chars"
      ],
      "metadata": {
        "id": "8XNz3Y6Y9LH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "def remove_accented_chars(x):\n",
        "    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return x"
      ],
      "metadata": {
        "id": "1INv4uw98t8X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'] = df['content'].apply(lambda x: remove_accented_chars(x))\n",
        "df['c_summary'] = df['c_summary'].apply(lambda x: remove_accented_chars(x))"
      ],
      "metadata": {
        "id": "uCDf8Gcb8_4u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Dataset"
      ],
      "metadata": {
        "id": "JqW3lWzpC6pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"new_NLP.csv\")"
      ],
      "metadata": {
        "id": "jSsspwYW9Try"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBz5WI4pwDJX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}